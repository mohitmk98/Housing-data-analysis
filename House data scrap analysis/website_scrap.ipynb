{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strictly read and follow comments for avoiding any inconvinience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from urllib.request import Request, urlopen\n",
    "import time\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the list beneath contains different user agents list to avoid getting pinned/tracked/banned while scraping\n",
    "user_agent_list = [\n",
    "   #Chrome\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 5.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 6.2; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36',\n",
    "    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36',\n",
    "    #Firefox\n",
    "    'Mozilla/4.0 (compatible; MSIE 9.0; Windows NT 6.1)',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko',\n",
    "    'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0)',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; Trident/7.0; rv:11.0) like Gecko',\n",
    "    'Mozilla/5.0 (Windows NT 6.2; WOW64; Trident/7.0; rv:11.0) like Gecko',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0) like Gecko',\n",
    "    'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.0; Trident/5.0)',\n",
    "    'Mozilla/5.0 (Windows NT 6.3; WOW64; Trident/7.0; rv:11.0) like Gecko',\n",
    "    'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0)',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; Win64; x64; Trident/7.0; rv:11.0) like Gecko',\n",
    "    'Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; WOW64; Trident/6.0)',\n",
    "    'Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; Trident/6.0)',\n",
    "    'Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0; .NET CLR 2.0.50727; .NET CLR 3.0.4506.2152; .NET CLR 3.5.30729)'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_number=''\n",
    "\n",
    "#this line generates a random number between 2 and 5 which is the amount of seconds the code stops for\n",
    "sleep_var=np.random.randint(3,5)\n",
    "\n",
    "#these are the lists the data will be stored in\n",
    "property_type_storage=[]\n",
    "property_price_storage=[]\n",
    "property_desc_storage=[]\n",
    "property_address_storage=[]\n",
    "property_refrence_storage=[]\n",
    "\n",
    "\n",
    "#this is the number of pages to scrap from the website, you can change it according to the preference\n",
    "num=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iterator in range(num):\n",
    "\n",
    "    url = 'https://www.homes.com/california-city-ca/homes-for-sale/'+page_number\n",
    "    \n",
    "     \n",
    "    #Pick a random user agent\n",
    "    user_agent = random.choice(user_agent_list)\n",
    "        \n",
    "    #this line makes sure the website doesn't block our crawler\n",
    "    \n",
    "    #setting headers\n",
    "    req = Request(url, headers={'User-Agent': user_agent})\n",
    "    \n",
    "    #Make the request\n",
    "    webpage = urlopen(req).read()\n",
    "    \n",
    "    #creating soup object\n",
    "    soup1 = BeautifulSoup(webpage, 'html.parser')\n",
    "    \n",
    "    property_type=soup1.find_all('div',class_='property-card-content__listing-type')\n",
    "    property_price=soup1.find_all('span',class_='details__price-val')\n",
    "    property_desc=soup1.find_all('div',class_='property-card-content__details')\n",
    "    property_address=soup1.find_all('h2',class_='property-card-content__address')\n",
    "    property_refrence=soup1.find_all('a',href=True,class_='content-click')\n",
    "    \n",
    "    \n",
    "    for i,j,k,l,m in zip(property_type,property_price,property_desc,property_address,property_refrence):\n",
    "        property_type_storage.append(i.get_text())\n",
    "        property_price_storage.append(j.get_text())\n",
    "        property_desc_storage.append(k.get_text())\n",
    "        property_address_storage.append(l.get_text())\n",
    "        property_refrence_storage.append('https://www.homes.com'+m['href'])\n",
    "\n",
    "       \n",
    "    page_number='p'+str(iterator+1)\n",
    "    time.sleep(sleep_var)\n",
    "    \n",
    "    #uncomment the line beneath to see progress\n",
    "    #print(\"number of pages fetched -\",iterator+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To create the dataframe of the fetched data\n",
    "detailed_dataframe=pd.DataFrame(list(zip(property_type_storage, property_price_storage,\n",
    "                            property_desc_storage, property_address_storage, property_refrence_storage)),columns=['property_type',\n",
    "                            'property_price', 'property_description','property_address','property_hyperlink'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop duplicate entries if any\n",
    "detailed_dataframe=detailed_dataframe.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to construct csv file which includes all the types of properties uncomment the lines beneath\n",
    "\n",
    "#detailed_dataframe=detailed_dataframe.replace(to_replace='-- acres',value='NaN')\n",
    "#detailed_dataframe.to_csv('property_data',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering information of residential houses/apartments whose details are present \n",
    "house_for_sale_data=detailed_dataframe[(detailed_dataframe['property_type']== 'house for sale') | (detailed_dataframe['property_type']== 'mobile manufactured for sale')]\n",
    "\n",
    "#to construct csv file which includes only houses for sale\n",
    "house_for_sale_data.to_csv('house_for_sale_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to make sure the lists are empty and there is no redundant data left, run this command at last\n",
    "property_type_storage.clear()\n",
    "property_price_storage.clear()\n",
    "property_desc_storage.clear()\n",
    "property_address_storage.clear()\n",
    "property_refrence_storage.clear()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.redfin.com/state/California\"\n",
    "\n",
    "\n",
    "#Pick a random user agent\n",
    "user_agent = random.choice(user_agent_list)\n",
    "\n",
    "#this line makes sure the website doesn't block our crawler\n",
    "\n",
    "#setting headers\n",
    "req = Request(url, headers={'User-Agent': user_agent})\n",
    "\n",
    "#Make the request\n",
    "webpage = urlopen(req).read()\n",
    "\n",
    "#creating soup object\n",
    "soup2 = BeautifulSoup(webpage, 'html.parser')\n",
    "\n",
    "\n",
    "page_number=''\n",
    "\n",
    "\n",
    "#these are the lists the data will be stored in\n",
    "property_type_storage=[]\n",
    "property_price_storage=[]\n",
    "property_desc_storage=[]\n",
    "property_address_storage=[]\n",
    "property_refrence_storage=[]\n",
    "\n",
    "\n",
    "#this is the number of pages to scrap from the website, you can change it according to the preference till 9\n",
    "num=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 403: Forbidden",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-419-7c3b2aa5077b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;31m#Make the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mwebpage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;31m#creating soup object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[0mopener\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    529\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    639\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    640\u001b[0m             response = self.parent.error(\n\u001b[1;32m--> 641\u001b[1;33m                 'http', request, response, code, msg, hdrs)\n\u001b[0m\u001b[0;32m    642\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    643\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36merror\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    567\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m             \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'default'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'http_error_default'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 569\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    570\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[1;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    501\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    502\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 503\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    504\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    647\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 649\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    650\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    651\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 403: Forbidden"
     ]
    }
   ],
   "source": [
    "city_href=[]\n",
    "for tag in soup2.findAll('a', href=True):\n",
    "    if 'city' in tag['href'] and 'https' not in tag['href']:\n",
    "        city_href.append('https://www.redfin.com'+tag['href'])\n",
    "        \n",
    "        \n",
    "for name in city_href: \n",
    "    for s in range(num):\n",
    "        #for iterator in range(num):\n",
    "\n",
    "        fetch_url=name+page_number\n",
    "\n",
    "\n",
    "        #Pick a random user agent\n",
    "        user_agent = random.choice(user_agent_list)\n",
    "\n",
    "        #this line makes sure the website doesn't block our crawler\n",
    "\n",
    "        #setting headers\n",
    "        req = Request(fetch_url, headers={'User-Agent': user_agent})\n",
    "\n",
    "        #Make the request\n",
    "        webpage = urlopen(req).read()\n",
    "\n",
    "        #creating soup object\n",
    "        temp_soup = BeautifulSoup(webpage, 'html.parser')\n",
    "\n",
    "\n",
    "\n",
    "        for i in range(20):\n",
    "            property_type_storage.append('house for sale')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        spans=temp_soup.find_all('span')\n",
    "        for span in spans:\n",
    "            if ',' in span.get_text() and '$' in span.get_text() and ' ' not in span.get_text():\n",
    "                property_price_storage.append(span.get_text())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        stats=temp_soup.find_all('div',class_='stats')\n",
    "\n",
    "        bed=[]\n",
    "        bath=[]\n",
    "        area=[]\n",
    "        for i in stats:\n",
    "            if 'bed' in i.get_text().lower():\n",
    "                bed.append(i.get_text())\n",
    "\n",
    "            elif 'bath' in i.get_text().lower():\n",
    "                bath.append(i.get_text())\n",
    "\n",
    "            else:\n",
    "                area.append(i.get_text())\n",
    "\n",
    "        for i, j, k in zip(bed,bath,area):\n",
    "            property_desc_storage.append(i+' | '+ j+' | '+ k) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        for span in spans:\n",
    "            if 'CA' in span.get_text():\n",
    "                property_address_storage.append(span.get_text())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        extract=temp_soup.find_all('a',href=True,attrs={\"data-rf-test-id\": \"slider-item-1\"})\n",
    "        for i in extract:\n",
    "            property_refrence_storage.append('https://www.redfin.com'+i['href'])\n",
    "\n",
    "        page_number='/page-'+ str(s+1)\n",
    "        sleep_var=np.random.randint(3,5)\n",
    "        time.sleep(sleep_var)\n",
    "\n",
    "    page_number=''\n",
    "\n",
    "    \n",
    "detailed_dataframe=pd.DataFrame(list(zip(property_type_storage, property_price_storage,\n",
    "                            property_desc_storage, property_address_storage, property_refrence_storage)),columns=['property_type',\n",
    "                            'property_price', 'property_description','property_address','property_hyperlink'])    \n",
    "#drop duplicate entries if any\n",
    "detailed_dataframe=detailed_dataframe.drop_duplicates()\n",
    "\n",
    "#filtering information of residential houses/apartments whose details are present \n",
    "house_for_sale_data=detailed_dataframe[(detailed_dataframe['property_type']== 'house for sale') | (detailed_dataframe['property_type']== 'mobile manufactured for sale')]\n",
    "\n",
    "#to construct csv file which includes only houses for sale\n",
    "house_for_sale_data.to_csv('house_for_sale_data.csv',index=False)\n",
    "\n",
    "\n",
    "#to make sure the lists are empty and there is no redundant data left, run this command at last\n",
    "property_type_storage.clear()\n",
    "property_price_storage.clear()\n",
    "property_desc_storage.clear()\n",
    "property_address_storage.clear()\n",
    "property_refrence_storage.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "    import random\n",
    "\n",
    "except:\n",
    "    print(\" Library Not Found !\")\n",
    "\n",
    "\n",
    "class Random_Proxy(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.__url = 'https://www.sslproxies.org/'\n",
    "        self.__headers = {\n",
    "            'Accept-Encoding': 'gzip, deflate, sdch',\n",
    "            'Accept-Language': 'en-US,en;q=0.8',\n",
    "            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Referer': 'http://www.wikipedia.org/',\n",
    "            'Connection': 'keep-alive',\n",
    "            }\n",
    "        self.random_ip = []\n",
    "        self.random_port = []\n",
    "\n",
    "    def __random_proxy(self):\n",
    "\n",
    "        \"\"\"\n",
    "        This is Private Function Client Should not have accesss\n",
    "        :return: Dictionary object of Random proxy and port number\n",
    "        \"\"\"\n",
    "\n",
    "        r = requests.get(url=self.__url, headers=self.__headers)\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "        # Get the Random IP Address\n",
    "        for x in soup.findAll('td')[::8]:\n",
    "            self.random_ip.append(x.get_text())\n",
    "\n",
    "        # Get Their Port\n",
    "        for y in soup.findAll('td')[1::8]:\n",
    "            self.random_port.append(y.get_text())\n",
    "\n",
    "        # Zip together\n",
    "        z = list(zip(self.random_ip, self.random_port))\n",
    "\n",
    "        # This will Fetch Random IP Address and corresponding PORT Number\n",
    "        number = random.randint(0, len(z)-50)\n",
    "        ip_random = z[number]\n",
    "\n",
    "        # convert Tuple into String and formart IP and PORT Address\n",
    "        ip_random_string = \"{}:{}\".format(ip_random[0],ip_random[1])\n",
    "\n",
    "        # Create a Proxy\n",
    "        proxy = {'https':ip_random_string}\n",
    "\n",
    "        # return Proxy\n",
    "        return proxy\n",
    "\n",
    "    def Proxy_Request(self,request_type='get',**kwargs):\n",
    "        \"\"\"\n",
    "\n",
    "        :param request_type: GET, POST, PUT\n",
    "        :param url: URL from which you want to do webscrapping\n",
    "        :param kwargs: any other parameter you pass\n",
    "        :return: Return Response\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            try:\n",
    "                proxy = self.__random_proxy()\n",
    "                return proxy\n",
    "                break\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
