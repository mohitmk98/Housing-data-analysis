{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from urllib.request import Request, urlopen\n",
    "import time\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Random_Proxy(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.__url = 'https://www.sslproxies.org/'\n",
    "        self.__headers = {\n",
    "            'Accept-Encoding': 'gzip, deflate, sdch',\n",
    "            'Accept-Language': 'en-US,en;q=0.8',\n",
    "            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Referer': 'http://www.wikipedia.org/',\n",
    "            'Connection': 'keep-alive',\n",
    "            }\n",
    "        self.random_ip = []\n",
    "        self.random_port = []\n",
    "\n",
    "    def __random_proxy(self):\n",
    "\n",
    "        \"\"\"\n",
    "        This is Private Function Client Should not have accesss\n",
    "        :return: Dictionary object of Random proxy and port number\n",
    "        \"\"\"\n",
    "\n",
    "        r = requests.get(url=self.__url, headers=self.__headers)\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "        # Get the Random IP Address\n",
    "        for x in soup.findAll('td')[::8]:\n",
    "            self.random_ip.append(x.get_text())\n",
    "\n",
    "        # Get Their Port\n",
    "        for y in soup.findAll('td')[1::8]:\n",
    "            self.random_port.append(y.get_text())\n",
    "\n",
    "        # Zip together\n",
    "        z = list(zip(self.random_ip, self.random_port))\n",
    "\n",
    "        # This will Fetch Random IP Address and corresponding PORT Number\n",
    "        number = random.randint(0, len(z)-50)\n",
    "        ip_random = z[number]\n",
    "\n",
    "        # convert Tuple into String and formart IP and PORT Address\n",
    "        ip_random_string = \"{}:{}\".format(ip_random[0],ip_random[1])\n",
    "\n",
    "        # Create a Proxy\n",
    "        proxy = {'https':ip_random_string}\n",
    "\n",
    "        # return Proxy\n",
    "        return proxy\n",
    "\n",
    "    def Proxy_Request(self,request_type='get',**kwargs):\n",
    "        \"\"\"\n",
    "\n",
    "        :param request_type: GET, POST, PUT\n",
    "        :param url: URL from which you want to do webscrapping\n",
    "        :param kwargs: any other parameter you pass\n",
    "        :return: Return Response\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            try:\n",
    "                proxy = self.__random_proxy()\n",
    "                return proxy\n",
    "                break\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "driver loaded\n",
      "link appended\n",
      "Time consuming: 10.084142684936523\n",
      "Time consuming: 4.005935192108154\n",
      "Time consuming: 10.177064180374146\n",
      "Time consuming: 9.567717790603638\n",
      "Time consuming: 6.123783349990845\n",
      "Time consuming: 10.039448261260986\n",
      "Time consuming: 10.045579433441162\n",
      "Time consuming: 8.147854804992676\n",
      "Time consuming: 10.043901443481445\n",
      "Time consuming: 10.218218088150024\n",
      "Time consuming: 7.662451267242432\n",
      "Time consuming: 5.88383412361145\n",
      "Time consuming: 7.061160564422607\n",
      "Time consuming: 5.283614873886108\n",
      "Time consuming: 2.739651679992676\n",
      "Time consuming: 7.421891689300537\n",
      "Time consuming: 6.807321310043335\n",
      "Time consuming: 3.0742220878601074\n",
      "Time consuming: 8.39832592010498\n",
      "Time consuming: 6.982191801071167\n",
      "Time consuming: 10.064590215682983\n",
      "Time consuming: 10.059587001800537\n",
      "Time consuming: 9.164369106292725\n",
      "Time consuming: 10.029670476913452\n",
      "Time consuming: 9.418977975845337\n",
      "Time consuming: 2.8700366020202637\n",
      "Time consuming: 6.438752889633179\n",
      "Time consuming: 8.752727270126343\n",
      "Time consuming: 6.372792959213257\n",
      "Time consuming: 6.819201707839966\n",
      "Time consuming: 8.751887559890747\n",
      "link appended\n",
      "Time consuming: 5.953511476516724\n",
      "Time consuming: 6.533665418624878\n",
      "Time consuming: 10.03508734703064\n",
      "Time consuming: 6.695360898971558\n",
      "Time consuming: 6.976292848587036\n",
      "Time consuming: 6.266651630401611\n",
      "Time consuming: 6.707659721374512\n",
      "Time consuming: 6.743504524230957\n",
      "Time consuming: 9.05042052268982\n",
      "Time consuming: 9.340084552764893\n",
      "Time consuming: 7.0758140087127686\n",
      "Time consuming: 7.0237414836883545\n",
      "Time consuming: 6.82287335395813\n",
      "Time consuming: 6.440442800521851\n",
      "Time consuming: 7.373687982559204\n",
      "Time consuming: 5.767909049987793\n",
      "Time consuming: 5.9167444705963135\n",
      "Time consuming: 6.270115375518799\n",
      "Time consuming: 6.373160362243652\n",
      "Time consuming: 10.03798532485962\n",
      "Time consuming: 10.07394528388977\n",
      "Time consuming: 10.026610374450684\n",
      "Time consuming: 6.390124797821045\n",
      "Time consuming: 7.411123514175415\n",
      "Time consuming: 10.326044082641602\n",
      "Time consuming: 10.083805561065674\n",
      "Time consuming: 10.021142482757568\n"
     ]
    }
   ],
   "source": [
    "property_address=[]\n",
    "location=[]\n",
    "property_type=[]\n",
    "property_desc=[]\n",
    "property_area=[]\n",
    "property_price=[]\n",
    "property_description=[]\n",
    "property_mls_info=[]\n",
    "property_href=[]\n",
    "near_schools=[]\n",
    "walkscore_url=[]\n",
    "walkscore=[]\n",
    "\n",
    "\n",
    "num=1\n",
    "url='https://www.mlslistings.com/Search/Result/fb6d557b-7c6c-49e1-a30e-aa735a962b7d/'\n",
    "\n",
    "driver=webdriver.Chrome(r'C:\\Users\\Mohit\\Desktop\\INTERNSHIP\\chromedriver_win32\\chromedriver.exe')\n",
    "\n",
    "print('driver loaded')\n",
    "\n",
    "for val in range(num,3):\n",
    "    \n",
    "    #setting headers\n",
    "    req = Request(url)\n",
    "\n",
    "    #Make the request\n",
    "    webpage = urlopen(req).read()\n",
    "\n",
    "    #creating soup object\n",
    "    soup2 = BeautifulSoup(webpage, 'html.parser')\n",
    "    \n",
    "    soup2.find_all('a',class_='search-nav-link',href=True)\n",
    "    \n",
    "    page_links=[]\n",
    "    for i in soup2.find_all('a',class_='search-nav-link',href=True):\n",
    "        if 'cal' in i.get_text().lower():\n",
    "            page_links.append('https://www.mlslistings.com'+i['href'])\n",
    "        \n",
    "    \n",
    "    print('link appended')\n",
    "    \n",
    "    for page in page_links:\n",
    "        url2=page\n",
    "        \n",
    "        t = time.time()\n",
    "        driver.set_page_load_timeout(10)\n",
    "\n",
    "        try:\n",
    "            driver.get(url2)\n",
    "        except TimeoutException:\n",
    "            driver.execute_script(\"window.stop();\")\n",
    "        print('Time consuming:', time.time() - t)\n",
    "        \n",
    "        driver.execute_script(\"window.scrollTo(0, 200);\")\n",
    "        time.sleep(0.5)\n",
    "        driver.execute_script(\"window.scrollTo(0, 400);\")\n",
    "        time.sleep(0.5)\n",
    "        driver.execute_script(\"window.scrollTo(0, 600);\")\n",
    "        time.sleep(0.5)\n",
    "        driver.execute_script(\"window.scrollTo(0, 800);\")\n",
    "        time.sleep(0.5)\n",
    "        driver.execute_script(\"window.scrollTo(0, 1000);\")\n",
    "        time.sleep(0.5)\n",
    "        driver.execute_script(\"window.scrollTo(0, 1200);\")\n",
    "        time.sleep(0.5)        \n",
    "        driver.execute_script(\"window.scrollTo(0, 1400);\")\n",
    "        time.sleep(0.5)\n",
    "        driver.execute_script(\"window.scrollTo(0, 1600);\")\n",
    "        time.sleep(0.5)\n",
    "        driver.execute_script(\"window.scrollTo(0, 1800);\")\n",
    "        time.sleep(0.5)\n",
    "        driver.execute_script(\"window.scrollTo(0, 2000);\")\n",
    "        time.sleep(0.5)\n",
    "        driver.execute_script(\"window.scrollTo(0, 2200);\")\n",
    "        time.sleep(0.5)\n",
    "        driver.execute_script(\"window.scrollTo(0, 2400);\")\n",
    "        time.sleep(0.5)\n",
    "        driver.execute_script(\"window.scrollTo(0, 2600);\")\n",
    "        time.sleep(0.5)\n",
    "        driver.execute_script(\"window.scrollTo(0, 2800);\")\n",
    "        time.sleep(0.5)        \n",
    "        driver.execute_script(\"window.scrollTo(0, 3000);\")\n",
    "        time.sleep(0.5)\n",
    "\n",
    "        webpage = driver.page_source\n",
    "\n",
    "        #creating soup object\n",
    "        extract_soup = BeautifulSoup(webpage, 'html.parser')\n",
    "        \n",
    "        property_address.append(str(extract_soup.find('h2').get_text()))\n",
    "        \n",
    "        \n",
    "        property_type.append(str(extract_soup.find(class_='font-size-base sm-sticky').get_text()))\n",
    "        \n",
    "        \n",
    "        for i in extract_soup.find_all('h4',class_='font-weight-bold h font-size-xl line-height-lg'):\n",
    "            property_price.append(str(i.get_text().split()[0]))\n",
    "\n",
    "        desc=[]\n",
    "        for i,j in zip(extract_soup.find_all('th'),range(5)):\n",
    "            desc.append(i.get_text().split())\n",
    "                \n",
    "        property_desc.append(str(desc[0][0])+' beds'+' | '+str(desc[1][0])+' bath'+' | '+'year built in: '+ str(desc[4][0]))\n",
    "        \n",
    "        property_area.append(str(desc[2][0]))\n",
    "        \n",
    "        prop=extract_soup.find_all(class_='mb-0 font-size-midr line-height-xl')\n",
    "        for i in prop:\n",
    "            property_description.append(i.get_text())\n",
    "            \n",
    "        \n",
    "        tags=extract_soup.find_all('p',class_='card-title font-weight-bold mb-0 font-size-midr line-height-xl')\n",
    "        info=extract_soup.find_all('p',class_='card-text font-size-midr line-height-xl')\n",
    "        \n",
    "        \n",
    "        property_tags_info=[]\n",
    "        for i,j in zip(tags,info):\n",
    "            property_tags_info.append(i.get_text()+' : '+j.get_text())\n",
    "        \n",
    "        property_mls_info.append(property_tags_info)\n",
    "        \n",
    "        \n",
    "        property_href.append(page)\n",
    "    \n",
    "        \n",
    "        driver.implicitly_wait(2)\n",
    "        try:\n",
    "            s1= driver.find_element_by_xpath('/html/body/div[1]/div[2]/div/div[4]/div[1]/div/div[1]/div/div/div[1]/div[5]/div[3]/div/div/div/div[2]/table/tbody')\n",
    "\n",
    "            near_schools.append(s1.text.split('\\n'))\n",
    "\n",
    "            s2=driver.find_element_by_xpath('/html/body/div[1]/div[2]/div/div[4]/div[1]/div/div[4]/div[2]/div/div/div/div[1]/div/div[1]/a[1]')\n",
    "            walkscore_href=str(s2.get_attribute(\"href\"))\n",
    "            walkscore_url.append(walkscore_href)\n",
    "        \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            try:\n",
    "                delay=5\n",
    "                myElem = WebDriverWait(driver, delay).until(EC.presence_of_element_located((By.ID, 'score-text')))\n",
    "                walk= driver.find_element_by_xpath('/html/body/div[1]/div[2]/div/div[4]/div[1]/div/div[4]/div[2]/div/div/div/div[1]/div/div[1]/span[1]')\n",
    "               \n",
    "                walkscore.append(walk.text)\n",
    "\n",
    "\n",
    "                lat=str(walkscore_href.split('/')[5])\n",
    "                long=str(walkscore_href.split('/')[6])\n",
    "                location.append(lat+' | '+long)\n",
    "\n",
    "\n",
    "            except TimeoutException:\n",
    "                walkscore.append('None')\n",
    "                location.append('None')\n",
    "\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        time.sleep(1)\n",
    "        \n",
    "    url=url+str(num+1)+'?view=list'\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "detailed_dataframe=pd.DataFrame(list(zip(\n",
    "property_type,\n",
    "property_price,\n",
    "property_address,\n",
    "property_desc,\n",
    "property_area,\n",
    "property_description,\n",
    "property_mls_info,\n",
    "property_href,\n",
    "location,\n",
    "near_schools,\n",
    "walkscore_url,\n",
    "walkscore)),\n",
    "columns=['property_type',\n",
    "         'property_price',\n",
    "         'property_address',\n",
    "         'property_details',\n",
    "         'property_area',\n",
    "         'property_description',\n",
    "         'property_mls_info',\n",
    "         'property_href',\n",
    "         'location',\n",
    "         'near_schools',\n",
    "         'walkscore_url',\n",
    "         'walkscore'])    \n",
    "\n",
    "#to construct csv file which includes only houses for sale\n",
    "detailed_dataframe.to_csv('mls_listings_for_california.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
